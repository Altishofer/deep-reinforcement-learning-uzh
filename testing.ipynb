{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-01T10:10:23.749061Z"
    }
   },
   "source": [
    "# %pip install procgen torch numpy tqdm matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "from procgen import ProcgenEnv\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    env_name: str = \"chaser\"\n",
    "    num_envs_train: int = 32\n",
    "    num_envs_eval: int = 32\n",
    "    num_levels_train: int = 200\n",
    "    num_levels_eval: int = 200\n",
    "    start_level_train: int = 0\n",
    "    start_level_eval: int = 10000\n",
    "    distribution_mode: str = \"easy\"\n",
    "    total_timesteps: int = 1_000_000\n",
    "    rollout_length: int = 256\n",
    "    update_epochs: int = 3\n",
    "    minibatch_size: int = 4096\n",
    "    gamma: float = 0.999\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    learning_rate: float = 5e-4\n",
    "    eval_interval_updates: int = 20\n",
    "    eval_steps_per_env: int = 512\n",
    "    seed: int = 1\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    use_random_shift: bool = True\n",
    "    shift_pad: int = 4\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_env(num_envs: int, env_name: str, num_levels: int, start_level: int, distribution_mode: str, rand_seed: int) -> ProcgenEnv:\n",
    "    return ProcgenEnv(\n",
    "        num_envs=num_envs,\n",
    "        env_name=env_name,\n",
    "        num_levels=num_levels,\n",
    "        start_level=start_level,\n",
    "        distribution_mode=distribution_mode,\n",
    "        rand_seed=rand_seed\n",
    "    )\n",
    "\n",
    "def get_obs(x):\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"obs\", \"observation\", \"rgb\"):\n",
    "            if k in x: return x[k]\n",
    "        raise KeyError(f\"reset/step returned dict without observation key: {list(x.keys())}\")\n",
    "    return x\n",
    "\n",
    "class RandomShift(nn.Module):\n",
    "    def __init__(self, pad: int = 4):\n",
    "        super().__init__()\n",
    "        self.pad = pad\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pad <= 0: return x\n",
    "        b, c, h, w = x.shape\n",
    "        x = F.pad(x, (self.pad, self.pad, self.pad, self.pad), mode=\"replicate\")\n",
    "        top = torch.randint(0, 2*self.pad + 1, (b,), device=x.device)\n",
    "        left = torch.randint(0, 2*self.pad + 1, (b,), device=x.device)\n",
    "        out = torch.empty((b, c, h, w), device=x.device, dtype=x.dtype)\n",
    "        for i in range(b):\n",
    "            out[i] = x[i, :, top[i]:top[i]+h, left[i]:left[i]+w]\n",
    "        return out\n",
    "\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, in_ch: int, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pi = nn.Linear(256, num_actions)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "    def forward(self, x):\n",
    "        z = self.conv(x)\n",
    "        z = self.fc(z)\n",
    "        return self.pi(z), self.v(z)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, cfg: Config):\n",
    "        set_seed(cfg.seed)\n",
    "        self.cfg = cfg\n",
    "        self.env = make_env(cfg.num_envs_train, cfg.env_name, cfg.num_levels_train, cfg.start_level_train, cfg.distribution_mode, rand_seed=cfg.seed)\n",
    "        self.eval_env = make_env(cfg.num_envs_eval, cfg.env_name, cfg.num_levels_eval, cfg.start_level_eval, cfg.distribution_mode, rand_seed=cfg.seed+123)\n",
    "        obs0_raw = self.env.reset()\n",
    "        obs0 = get_obs(obs0_raw)\n",
    "        self.N = obs0.shape[0]\n",
    "        self.H, self.W, self.C = obs0.shape[1], obs0.shape[2], obs0.shape[3]\n",
    "        self.num_actions = self.env.action_space.n if hasattr(self.env, \"action_space\") else 15\n",
    "        self.model = CNNPolicy(in_ch=self.C, num_actions=self.num_actions).to(cfg.device)\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=cfg.learning_rate, eps=1e-5)\n",
    "        self.augment = RandomShift(cfg.shift_pad) if cfg.use_random_shift else None\n",
    "        self.global_step = 0\n",
    "        self.metrics: Dict[str, List[float]] = {\n",
    "            \"updates\": [], \"train_return_mean\": [], \"train_return_std\": [],\n",
    "            \"eval_return_mean\": [], \"eval_return_std\": [],\n",
    "            \"policy_loss\": [], \"value_loss\": [], \"entropy\": []\n",
    "        }\n",
    "        self.last_obs = obs0\n",
    "\n",
    "    def _prep(self, obs_np: np.ndarray) -> torch.Tensor:\n",
    "        x = torch.from_numpy(obs_np).to(self.cfg.device).float() / 255.0\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "    def _policy(self, x: torch.Tensor):\n",
    "        if self.augment is not None and self.model.training:\n",
    "            x = self.augment(x)\n",
    "        logits, value = self.model(x)\n",
    "        dist = Categorical(logits=logits)\n",
    "        return dist, value.squeeze(-1)\n",
    "\n",
    "    def _env_step(self, env: ProcgenEnv, action_np: np.ndarray):\n",
    "        out = env.step(action_np)\n",
    "        if len(out) == 4:\n",
    "            obs, rew, done, info = out\n",
    "        elif len(out) == 5:\n",
    "            obs, rew, done, info, _ = out\n",
    "        else:\n",
    "            raise RuntimeError(\"Unexpected ProcgenEnv.step output format\")\n",
    "        obs = get_obs(obs)\n",
    "        return obs, rew, done, info\n",
    "\n",
    "    def collect(self):\n",
    "        T = self.cfg.rollout_length\n",
    "        N = self.cfg.num_envs_train\n",
    "        obs = self.last_obs if self.last_obs is not None else get_obs(self.env.reset())\n",
    "\n",
    "        obs_buf = torch.zeros((T, N, self.C, self.H, self.W), device=self.cfg.device)\n",
    "        act_buf = torch.zeros((T, N), device=self.cfg.device, dtype=torch.long)\n",
    "        logp_buf = torch.zeros((T, N), device=self.cfg.device)\n",
    "        rew_buf = torch.zeros((T, N), device=self.cfg.device)\n",
    "        done_buf = torch.zeros((T, N), device=self.cfg.device)\n",
    "        val_buf = torch.zeros((T, N), device=self.cfg.device)\n",
    "\n",
    "        ep_returns = np.zeros(N, dtype=np.float32)\n",
    "        finished_returns = []\n",
    "\n",
    "        for t in range(T):\n",
    "            self.global_step += N\n",
    "            x = self._prep(obs)\n",
    "            with torch.no_grad():\n",
    "                self.model.train()\n",
    "                dist, value = self._policy(x)\n",
    "                action = dist.sample()\n",
    "                logp = dist.log_prob(action)\n",
    "            next_obs, reward, done, info = self._env_step(self.env, action.detach().cpu().numpy())\n",
    "            obs_buf[t] = x\n",
    "            act_buf[t] = action\n",
    "            logp_buf[t] = logp\n",
    "            rew_buf[t] = torch.from_numpy(reward).to(self.cfg.device)\n",
    "            done_buf[t] = torch.from_numpy(done.astype(np.float32)).to(self.cfg.device)\n",
    "            val_buf[t] = value\n",
    "            ep_returns += reward\n",
    "            for i in range(N):\n",
    "                if done[i]:\n",
    "                    finished_returns.append(ep_returns[i])\n",
    "                    ep_returns[i] = 0.0\n",
    "            obs = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_last = self._prep(obs)\n",
    "            self.model.train()\n",
    "            _, next_value = self._policy(x_last)\n",
    "\n",
    "        adv_buf = torch.zeros_like(rew_buf)\n",
    "        lastgaelam = torch.zeros((N,), device=self.cfg.device)\n",
    "        for t in reversed(range(T)):\n",
    "            nextnonterm = 1.0 - done_buf[t]\n",
    "            nextv = val_buf[t+1] if t < T-1 else next_value\n",
    "            delta = rew_buf[t] + self.cfg.gamma * nextv * nextnonterm - val_buf[t]\n",
    "            lastgaelam = delta + self.cfg.gamma * self.cfg.gae_lambda * nextnonterm * lastgaelam\n",
    "            adv_buf[t] = lastgaelam\n",
    "        ret_buf = adv_buf + val_buf\n",
    "\n",
    "        self.last_obs = obs\n",
    "\n",
    "        tr_mean = float(np.mean(finished_returns)) if len(finished_returns) > 0 else 0.0\n",
    "        tr_std = float(np.std(finished_returns)) if len(finished_returns) > 0 else 0.0\n",
    "        self.metrics[\"train_return_mean\"].append(tr_mean)\n",
    "        self.metrics[\"train_return_std\"].append(tr_std)\n",
    "\n",
    "        return obs_buf, act_buf, logp_buf, adv_buf, ret_buf\n",
    "\n",
    "    def update(self, obs_buf, act_buf, logp_buf, adv_buf, ret_buf):\n",
    "        T, N = obs_buf.shape[0], obs_buf.shape[1]\n",
    "        B = T * N\n",
    "        obs = obs_buf.reshape(B, self.C, self.H, self.W)\n",
    "        act = act_buf.reshape(B)\n",
    "        old_logp = logp_buf.reshape(B)\n",
    "        adv = adv_buf.reshape(B)\n",
    "        ret = ret_buf.reshape(B)\n",
    "        adv = (adv - adv.mean()) / (adv.std(unbiased=False) + 1e-8)\n",
    "\n",
    "        inds = np.arange(B)\n",
    "        pl_losses, v_losses, ents = [], [], []\n",
    "\n",
    "        for _ in range(self.cfg.update_epochs):\n",
    "            np.random.shuffle(inds)\n",
    "            for start in range(0, B, self.cfg.minibatch_size):\n",
    "                end = start + self.cfg.minibatch_size\n",
    "                mb = inds[start:end]\n",
    "                self.model.train()\n",
    "                dist, value = self._policy(obs[mb])\n",
    "                new_logp = dist.log_prob(act[mb])\n",
    "                entropy = dist.entropy().mean()\n",
    "                ratio = (new_logp - old_logp[mb]).exp()\n",
    "                pg_loss = torch.max(-adv[mb] * ratio,\n",
    "                                    -adv[mb] * torch.clamp(ratio, 1.0 - self.cfg.clip_coef, 1.0 + self.cfg.clip_coef)).mean()\n",
    "                v_loss = F.mse_loss(value, ret[mb])\n",
    "                loss = pg_loss + self.cfg.vf_coef * v_loss - self.cfg.ent_coef * entropy\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.max_grad_norm)\n",
    "                self.opt.step()\n",
    "                pl_losses.append(pg_loss.item())\n",
    "                v_losses.append(v_loss.item())\n",
    "                ents.append(entropy.item())\n",
    "\n",
    "        self.metrics[\"policy_loss\"].append(float(np.mean(pl_losses)))\n",
    "        self.metrics[\"value_loss\"].append(float(np.mean(v_losses)))\n",
    "        self.metrics[\"entropy\"].append(float(np.mean(ents)))\n",
    "\n",
    "    def evaluate(self) -> Tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        obs = get_obs(self.eval_env.reset())\n",
    "        N = self.cfg.num_envs_eval\n",
    "        steps = self.cfg.eval_steps_per_env\n",
    "        ep_returns = np.zeros(N, dtype=np.float32)\n",
    "        finished = []\n",
    "        for _ in range(steps):\n",
    "            with torch.no_grad():\n",
    "                x = self._prep(obs)\n",
    "                logits, _ = self.model(x)\n",
    "                action = torch.argmax(logits, dim=-1)\n",
    "            obs, reward, done, info = self._env_step(self.eval_env, action.cpu().numpy())\n",
    "            ep_returns += reward\n",
    "            for i in range(N):\n",
    "                if done[i]:\n",
    "                    finished.append(ep_returns[i])\n",
    "                    ep_returns[i] = 0.0\n",
    "        m = float(np.mean(finished)) if len(finished) > 0 else float(np.mean(ep_returns))\n",
    "        s = float(np.std(finished)) if len(finished) > 0 else float(np.std(ep_returns))\n",
    "        self.metrics[\"eval_return_mean\"].append(m)\n",
    "        self.metrics[\"eval_return_std\"].append(s)\n",
    "        return m, s\n",
    "\n",
    "    def train(self):\n",
    "        num_updates = self.cfg.total_timesteps // (self.cfg.num_envs_train * self.cfg.rollout_length)\n",
    "        pbar = tqdm(range(num_updates), desc=\"PPO\")\n",
    "        for u in pbar:\n",
    "            obs_buf, act_buf, logp_buf, adv_buf, ret_buf = self.collect()\n",
    "            self.update(obs_buf, act_buf, logp_buf, adv_buf, ret_buf)\n",
    "            if (u + 1) % self.cfg.eval_interval_updates == 0:\n",
    "                ev_m, _ = self.evaluate()\n",
    "            else:\n",
    "                ev_m = self.metrics[\"eval_return_mean\"][-1] if self.metrics[\"eval_return_mean\"] else 0.0\n",
    "            self.metrics[\"updates\"].append(u + 1)\n",
    "            pbar.set_postfix(train=f\"{self.metrics['train_return_mean'][-1]:.1f}\", eval=f\"{ev_m:.1f}\")\n",
    "\n",
    "def plot_metrics(metrics: Dict[str, List[float]], title=\"Procgen Chaser - Generalization\"):\n",
    "    up = metrics[\"updates\"]\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    axs[0].plot(up, metrics[\"train_return_mean\"], label=\"train\")\n",
    "    axs[0].fill_between(up,\n",
    "                        np.array(metrics[\"train_return_mean\"]) - np.array(metrics[\"train_return_std\"]),\n",
    "                        np.array(metrics[\"train_return_mean\"]) + np.array(metrics[\"train_return_std\"]),\n",
    "                        alpha=0.2)\n",
    "    axs[0].set_title(\"Train return\")\n",
    "    axs[0].set_xlabel(\"Update\")\n",
    "    axs[0].set_ylabel(\"Return\")\n",
    "    axs[1].plot(up, metrics[\"eval_return_mean\"], label=\"eval\", color=\"green\")\n",
    "    if len(metrics[\"eval_return_std\"]) == len(up):\n",
    "        axs[1].fill_between(up,\n",
    "                            np.array(metrics[\"eval_return_mean\"]) - np.array(metrics[\"eval_return_std\"]),\n",
    "                            np.array(metrics[\"eval_return_mean\"]) + np.array(metrics[\"eval_return_std\"]),\n",
    "                            color=\"green\", alpha=0.2)\n",
    "    axs[1].set_title(\"Eval return (unseen)\")\n",
    "    axs[1].set_xlabel(\"Update\")\n",
    "    axs[2].plot(up, metrics[\"policy_loss\"], label=\"pi loss\", color=\"red\")\n",
    "    axs[2].plot(up, metrics[\"value_loss\"], label=\"v loss\", color=\"orange\")\n",
    "    axs[2].set_title(\"Losses\")\n",
    "    axs[2].set_xlabel(\"Update\")\n",
    "    axs[2].legend()\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config()\n",
    "    agent = PPOAgent(cfg)\n",
    "    agent.train()\n",
    "    plot_metrics(agent.metrics, title=\"CNN + random shift (minimal)\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO:   0%|          | 0/122 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
