{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sit_tiny import SietTiny\n",
    "\n",
    "class TinySieTPolicy(nn.Module):\n",
    "    def __init__(self, in_ch, num_actions, h, w,\n",
    "                 img_size=64, embed_dim=32, depth=1, num_heads=8,\n",
    "                 patch_size=8, patch_size_local=8, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        assert h == img_size and w == img_size, \"SietTiny img_size must match observation H and W\"\n",
    "        self.backbone = SietTiny(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            patch_size_local=patch_size_local,\n",
    "            in_chans=in_ch,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_ch, h, w)\n",
    "            z = self.backbone(dummy)\n",
    "            feat_dim = z.shape[-1]\n",
    "        self.pi = nn.Linear(feat_dim, num_actions)\n",
    "        self.v = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.backbone(x)\n",
    "        return self.pi(z), self.v(z).squeeze(-1)\n"
   ],
   "id": "f180fd35f95971b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T11:58:15.282889Z",
     "start_time": "2025-10-08T11:58:14.810026Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ToBaselinesVecEnv' object has no attribute 'num_actions'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 142\u001B[0m\n\u001B[0;32m    139\u001B[0m     ppo_train(env_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoinrun\u001B[39m\u001B[38;5;124m\"\u001B[39m, total_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1_000_000\u001B[39m, num_envs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, rollout_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 142\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 139\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mmain\u001B[39m():\n\u001B[0;32m    138\u001B[0m     device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 139\u001B[0m     \u001B[43mppo_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcoinrun\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1_000_000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_envs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrollout_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 69\u001B[0m, in \u001B[0;36mppo_train\u001B[1;34m(env_name, total_steps, num_envs, rollout_len, gamma, lam, clip_eps, vf_coef, ent_coef, lr, minibatch_size, update_epochs, device)\u001B[0m\n\u001B[0;32m     67\u001B[0m obs \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m     68\u001B[0m obs_t \u001B[38;5;241m=\u001B[39m to_torch(obs, device)\n\u001B[1;32m---> 69\u001B[0m n_actions \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_actions\u001B[49m\n\u001B[0;32m     70\u001B[0m obs_shape \u001B[38;5;241m=\u001B[39m obs_t\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m     71\u001B[0m policy \u001B[38;5;241m=\u001B[39m CNNPolicy(obs_shape, n_actions)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ToBaselinesVecEnv' object has no attribute 'num_actions'"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "import os, math, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from procgen import ProcgenEnv\n",
    "\n",
    "class OrthogonalInit:\n",
    "    def __call__(self, m, gain=1.0):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.orthogonal_(m.weight, gain=gain)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.orthogonal_(m.weight, gain=gain)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super().__init__()\n",
    "        c = obs_shape[0]\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *obs_shape)\n",
    "            feat_dim = self.net(dummy).shape[1]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Linear(256, n_actions)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "        OrthogonalInit()(self.net)\n",
    "        OrthogonalInit()(self.mlp, gain=1.0)\n",
    "        OrthogonalInit()(self.policy, gain=0.01)\n",
    "        OrthogonalInit()(self.value, gain=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0\n",
    "        z = self.mlp(self.net(x))\n",
    "        return self.policy(z), self.value(z)\n",
    "\n",
    "    def act(self, x):\n",
    "        logits, v = self(x)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        a = dist.sample()\n",
    "        return a, dist.log_prob(a), dist.entropy(), v.squeeze(-1)\n",
    "\n",
    "def make_env(num_envs, env_name=\"coinrun\", num_levels=0, start_level=0, distribution_mode=\"easy\"):\n",
    "    env = ProcgenEnv(num_envs=num_envs, env_name=env_name, num_levels=num_levels, start_level=start_level, distribution_mode=distribution_mode)\n",
    "    return env\n",
    "\n",
    "def to_torch(x, device):\n",
    "    if isinstance(x, dict):\n",
    "        x = x[\"rgb\"]\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2).to(device)\n",
    "    return x\n",
    "\n",
    "def ppo_train(env_name=\"coinrun\", total_steps=2_000_000, num_envs=64, rollout_len=256, gamma=0.999, lam=0.95, clip_eps=0.2, vf_coef=0.5, ent_coef=0.01, lr=2.5e-4, minibatch_size=8192, update_epochs=3, device=\"cuda\"):\n",
    "    env = make_env(num_envs=num_envs, env_name=env_name)\n",
    "    obs = env.reset()\n",
    "    obs_t = to_torch(obs, device)\n",
    "    n_actions = env.num_actions\n",
    "    obs_shape = obs_t.shape[1:]\n",
    "    policy = CNNPolicy(obs_shape, n_actions).to(device)\n",
    "    opt = optim.Adam(policy.parameters(), lr=lr, eps=1e-5)\n",
    "    steps = 0\n",
    "    rollout_obs = torch.zeros(rollout_len, num_envs, *obs_shape, device=device, dtype=torch.uint8)\n",
    "    rollout_actions = torch.zeros(rollout_len, num_envs, device=device, dtype=torch.long)\n",
    "    rollout_logprobs = torch.zeros(rollout_len, num_envs, device=device)\n",
    "    rollout_values = torch.zeros(rollout_len, num_envs, device=device)\n",
    "    rollout_rewards = torch.zeros(rollout_len, num_envs, device=device)\n",
    "    rollout_dones = torch.zeros(rollout_len, num_envs, device=device)\n",
    "\n",
    "    while steps < total_steps:\n",
    "        for t in range(rollout_len):\n",
    "            rollout_obs[t] = to_torch(obs, device)\n",
    "            with torch.no_grad():\n",
    "                a, lp, ent, v = policy.act(rollout_obs[t])\n",
    "            rollout_actions[t] = a\n",
    "            rollout_logprobs[t] = lp\n",
    "            rollout_values[t] = v\n",
    "            obs, rew, done, info = env.step(a.cpu().numpy())\n",
    "            rollout_rewards[t] = torch.from_numpy(rew).to(device)\n",
    "            rollout_dones[t] = torch.from_numpy(done.astype(np.float32)).to(device)\n",
    "        with torch.no_grad():\n",
    "            next_value = policy(rollout_obs[-1].float() / 255.0)[1].squeeze(-1)\n",
    "        adv = torch.zeros(rollout_len, num_envs, device=device)\n",
    "        gae = torch.zeros(num_envs, device=device)\n",
    "        returns = torch.zeros(rollout_len, num_envs, device=device)\n",
    "        for t in reversed(range(rollout_len)):\n",
    "            nonterminal = 1.0 - rollout_dones[t]\n",
    "            delta = rollout_rewards[t] + gamma * (next_value if t == rollout_len - 1 else rollout_values[t + 1]) * nonterminal - rollout_values[t]\n",
    "            gae = delta + gamma * lam * nonterminal * gae\n",
    "            adv[t] = gae\n",
    "            returns[t] = adv[t] + rollout_values[t]\n",
    "        b_obs = rollout_obs.reshape(rollout_len * num_envs, *obs_shape).float() / 255.0\n",
    "        b_actions = rollout_actions.reshape(-1)\n",
    "        b_logprobs = rollout_logprobs.reshape(-1)\n",
    "        b_values = rollout_values.reshape(-1)\n",
    "        b_adv = adv.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_adv = (b_adv - b_adv.mean()) / (b_adv.std(unbiased=False) + 1e-8)\n",
    "        num_batch = b_obs.shape[0]\n",
    "        for _ in range(update_epochs):\n",
    "            idx = torch.randperm(num_batch, device=device)\n",
    "            for start in range(0, num_batch, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb = idx[start:end]\n",
    "                logits, v = policy(b_obs[mb])\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                lp = dist.log_prob(b_actions[mb])\n",
    "                ratio = torch.exp(lp - b_logprobs[mb])\n",
    "                surr1 = ratio * b_adv[mb]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * b_adv[mb]\n",
    "                pg_loss = -torch.min(surr1, surr2).mean()\n",
    "                v_pred = v.squeeze(-1)\n",
    "                v_clipped = b_values[mb] + torch.clamp(v_pred - b_values[mb], -clip_eps, clip_eps)\n",
    "                vf_loss1 = (v_pred - b_returns[mb]).pow(2)\n",
    "                vf_loss2 = (v_clipped - b_returns[mb]).pow(2)\n",
    "                vf_loss = 0.5 * torch.max(vf_loss1, vf_loss2).mean()\n",
    "                ent = dist.entropy().mean()\n",
    "                loss = pg_loss + vf_coef * vf_loss - ent_coef * ent\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(policy.parameters(), 0.5)\n",
    "                opt.step()\n",
    "        steps += rollout_len * num_envs\n",
    "        obs = env.reset()\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ppo_train(env_name=\"coinrun\", total_steps=1_000_000, num_envs=64, rollout_len=256, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "2fdf36ef96cfaec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
