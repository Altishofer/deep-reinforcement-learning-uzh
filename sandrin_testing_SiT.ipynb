{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T09:33:05.699966Z",
     "start_time": "2025-10-04T09:33:02.573407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from procgen import ProcgenEnv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reuse SiT (SietTiny) as the shared encoder for PPO\n",
    "# Assumes SietTiny is defined in sit_model.py (the file you shared)\n",
    "from SiT_model import SietTiny\n",
    "\n",
    "class DummyAugment:\n",
    "    def __call__(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return obs\n",
    "\n",
    "class SiTPPO(nn.Module):\n",
    "    def __init__(self, action_space_n: int, img_size=64, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder = SietTiny(img_size=img_size, patch_size=8, patch_size_local=8, in_chans=3,\n",
    "                                embed_dim=32, depth=1, num_heads=8)\n",
    "        # SiT encoder returns a flat embedding vector per obs; infer dim with a dummy\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, img_size, img_size)\n",
    "            enc_dim = self.encoder(dummy).shape[-1]\n",
    "        self.pi = nn.Linear(enc_dim, action_space_n)\n",
    "        self.v = nn.Linear(enc_dim, 1)\n",
    "        self.augment = DummyAugment()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def encode(self, obs_np: np.ndarray) -> torch.Tensor:\n",
    "        x = torch.from_numpy(obs_np).float().to(self.device) / 255.0\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.augment(x)\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def policy_value(self, obs_np: np.ndarray):\n",
    "        z = self.encode(obs_np)\n",
    "        logits = self.pi(z)\n",
    "        value = self.v(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "def make_env(num_envs=8, env_name=\"coinrun\", num_levels=0, start_level=0, render=False):\n",
    "    env = ProcgenEnv(num_envs=num_envs, env_name=env_name, start_level=start_level,\n",
    "                     num_levels=num_levels, distribution_mode=\"easy\")\n",
    "    return env\n",
    "\n",
    "def ppo_train():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_envs = 16\n",
    "    num_steps = 256\n",
    "    total_updates = 800\n",
    "    gamma = 0.999\n",
    "    gae_lambda = 0.95\n",
    "    clip_coef = 0.2\n",
    "    ent_coef = 0.01\n",
    "    vf_coef = 0.5\n",
    "    max_grad_norm = 0.5\n",
    "    lr = 2.5e-4\n",
    "    minibatch_size = 2048\n",
    "    env_name = \"coinrun\"\n",
    "    img_size = 64\n",
    "\n",
    "    env = make_env(num_envs=num_envs, env_name=env_name)\n",
    "    obs_space = env.observation_space\n",
    "    act_space = env.action_space\n",
    "    assert isinstance(act_space, gym.spaces.Discrete)\n",
    "    agent = SiTPPO(act_space.n, img_size=img_size, device=device)\n",
    "\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)\n",
    "\n",
    "    obs = env.reset()\n",
    "    ep_returns = np.zeros(num_envs, dtype=np.float32)\n",
    "    logged_returns = []\n",
    "    losses = []\n",
    "\n",
    "    for update in range(total_updates):\n",
    "        obs_buf = np.zeros((num_steps, num_envs, img_size, img_size, 3), dtype=np.uint8)\n",
    "        act_buf = np.zeros((num_steps, num_envs), dtype=np.int64)\n",
    "        logp_buf = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        rew_buf = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        val_buf = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        done_buf = np.zeros((num_steps, num_envs), dtype=np.bool_)\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            logits, value = agent.policy_value(obs)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            logp = dist.log_prob(action)\n",
    "\n",
    "            obs_buf[t] = obs\n",
    "            act_buf[t] = action.cpu().numpy()\n",
    "            logp_buf[t] = logp.detach().cpu().numpy()\n",
    "            val_buf[t] = value.detach().cpu().numpy()\n",
    "\n",
    "            obs, rew, done, info = env.step(action.cpu().numpy())\n",
    "            rew_buf[t] = rew\n",
    "            done_buf[t] = done\n",
    "\n",
    "            ep_returns += rew\n",
    "            for i, d in enumerate(done):\n",
    "                if d:\n",
    "                    logged_returns.append(ep_returns[i])\n",
    "                    ep_returns[i] = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_logits, next_value = agent.policy_value(obs)\n",
    "            next_value = next_value.cpu().numpy()\n",
    "\n",
    "        adv_buf = np.zeros_like(rew_buf)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            next_nonterminal = 1.0 - done_buf[t].astype(np.float32)\n",
    "            next_values = next_value if t == num_steps - 1 else val_buf[t + 1]\n",
    "            delta = rew_buf[t] + gamma * next_values * next_nonterminal - val_buf[t]\n",
    "            lastgaelam = delta + gamma * gae_lambda * next_nonterminal * lastgaelam\n",
    "            adv_buf[t] = lastgaelam\n",
    "        ret_buf = adv_buf + val_buf\n",
    "        adv_flat = adv_buf.reshape(-1)\n",
    "        adv_flat = (adv_flat - adv_flat.mean()) / (adv_flat.std() + 1e-8)\n",
    "\n",
    "        b_obs = obs_buf.reshape(num_steps * num_envs, img_size, img_size, 3)\n",
    "        b_act = act_buf.reshape(-1)\n",
    "        b_logp = logp_buf.reshape(-1)\n",
    "        b_adv = adv_flat\n",
    "        b_ret = ret_buf.reshape(-1)\n",
    "        b_val = val_buf.reshape(-1)\n",
    "\n",
    "        inds = np.arange(b_obs.shape[0])\n",
    "        np.random.shuffle(inds)\n",
    "\n",
    "        total_loss_epoch = 0.0\n",
    "        for start in range(0, len(inds), minibatch_size):\n",
    "            mb_inds = inds[start:start + minibatch_size]\n",
    "            mb_obs = b_obs[mb_inds]\n",
    "            mb_act = torch.from_numpy(b_act[mb_inds]).to(agent.device)\n",
    "            mb_adv = torch.from_numpy(b_adv[mb_inds]).float().to(agent.device)\n",
    "            mb_ret = torch.from_numpy(b_ret[mb_inds]).float().to(agent.device)\n",
    "            mb_logp_old = torch.from_numpy(b_logp[mb_inds]).float().to(agent.device)\n",
    "\n",
    "            logits, value = agent.policy_value(mb_obs)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            logp = dist.log_prob(mb_act)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratio = torch.exp(logp - mb_logp_old)\n",
    "            surr1 = ratio * mb_adv\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef) * mb_adv\n",
    "            pg_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            v_loss = 0.5 * (mb_ret - value).pow(2).mean()\n",
    "            loss = pg_loss + vf_coef * v_loss - ent_coef * entropy\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_epoch += loss.item()\n",
    "\n",
    "        avg_loss = total_loss_epoch / max(1, len(inds) // minibatch_size)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if (update + 1) % 10 == 0:\n",
    "            print(f\"Update {update+1}/{total_updates} | AvgLoss {avg_loss:.4f} | MeanReturn {np.mean(logged_returns[-100:]) if logged_returns else 0:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(losses, label=\"Training Loss\")\n",
    "    if len(logged_returns) > 0:\n",
    "        window = 50\n",
    "        returns_smoothed = np.convolve(logged_returns, np.ones(window)/window, mode=\"valid\")\n",
    "        plt.plot(np.linspace(0, len(losses), len(returns_smoothed)), returns_smoothed, label=f\"Return MA({window})\")\n",
    "    plt.xlabel(\"Update\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.legend()\n",
    "    plt.title(\"PPO with SiT on Procgen\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ppo_train()\n"
   ],
   "id": "3953b535e9af97a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 16, 4, 8]' is invalid for input of size 8192",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 184\u001B[0m\n\u001B[0;32m    181\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 184\u001B[0m     \u001B[43mppo_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 69\u001B[0m, in \u001B[0;36mppo_train\u001B[1;34m()\u001B[0m\n\u001B[0;32m     67\u001B[0m act_space \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(act_space, gym\u001B[38;5;241m.\u001B[39mspaces\u001B[38;5;241m.\u001B[39mDiscrete)\n\u001B[1;32m---> 69\u001B[0m agent \u001B[38;5;241m=\u001B[39m \u001B[43mSiTPPO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mact_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(agent\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m)\n\u001B[0;32m     73\u001B[0m obs \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "Cell \u001B[1;32mIn[1], line 26\u001B[0m, in \u001B[0;36mSiTPPO.__init__\u001B[1;34m(self, action_space_n, img_size, device)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     25\u001B[0m     dummy \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, img_size, img_size)\n\u001B[1;32m---> 26\u001B[0m     enc_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdummy\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(enc_dim, action_space_n)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(enc_dim, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\SiT_model.py:892\u001B[0m, in \u001B[0;36mSietTiny.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    890\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(x)\n\u001B[0;32m    891\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m--> 892\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreshape(bs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_patches, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim)\n\u001B[0;32m    894\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvit_global(x)\n\u001B[0;32m    895\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\SiT_model.py:652\u001B[0m, in \u001B[0;36mSym_Break_Linear_Block.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    649\u001B[0m qkv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_patches_flat(torch\u001B[38;5;241m.\u001B[39mcat([q, k, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprojx(x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)), ], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m    650\u001B[0m                                       ps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m    651\u001B[0m q, k, v \u001B[38;5;241m=\u001B[39m qkv[:, :, :f], qkv[:, :, f:\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m f], qkv[:, :, \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m f:]\n\u001B[1;32m--> 652\u001B[0m x \u001B[38;5;241m=\u001B[39m v \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    653\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreconstruct_image(\n\u001B[0;32m    654\u001B[0m     x\u001B[38;5;241m.\u001B[39mreshape(bs, ((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39midim \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m)) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, f),\n\u001B[0;32m    655\u001B[0m     ps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, img_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39midim \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    656\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\SiT_model.py:746\u001B[0m, in \u001B[0;36mAttention_Basic.forward\u001B[1;34m(self, q, k, v)\u001B[0m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, q, k, v):\n\u001B[0;32m    745\u001B[0m     B, N, C \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m--> 746\u001B[0m     q \u001B[38;5;241m=\u001B[39m \u001B[43mq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m    747\u001B[0m     k \u001B[38;5;241m=\u001B[39m k\u001B[38;5;241m.\u001B[39mreshape(B, N, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, C \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m    748\u001B[0m     v \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mreshape(B, N, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, C \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[64, 16, 4, 8]' is invalid for input of size 8192"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
