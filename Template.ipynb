{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T11:13:52.928206Z",
     "start_time": "2025-10-08T11:13:52.918377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from SiT_model import SietTiny\n",
    "\n",
    "\n",
    "class SietBackbone(nn.Module):\n",
    "    def __init__(self, in_chans=4, img_size=84, patch_size=6, patch_size_local=6,\n",
    "                 embed_dim=32, num_heads=8, depth=1):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.img_size = img_size\n",
    "        self.core = SietTiny(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            patch_size_local=patch_size_local,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.core(x)\n"
   ],
   "id": "1b65de6d6733ba71",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T11:13:53.248410Z",
     "start_time": "2025-10-08T11:13:52.939548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from procgen import ProcgenEnv\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, nb_actions):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 8, stride=4), nn.Tanh(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2), nn.Tanh(),\n",
    "            nn.Flatten(), nn.Linear(2592, 256), nn.Tanh(),\n",
    "        )\n",
    "        self.actor = nn.Sequential(nn.Linear(256, nb_actions))\n",
    "        self.critic = nn.Sequential(nn.Linear(256, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.head(x)\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "\n",
    "class SimpleProcgenWrapper:\n",
    "    def __init__(self, game_name=\"miner\", num_envs=1, distribution_mode=\"easy\",  skip=4, frame_stack=4, resize=(84, 84), grayscale=True, start_level=0):\n",
    "        self.env = ProcgenEnv(num_envs=num_envs, env_name=game_name, distribution_mode=distribution_mode, start_level=start_level)\n",
    "        self.env.reset()\n",
    "        self.num_envs = num_envs\n",
    "        self.skip = skip\n",
    "        self.frame_stack = frame_stack\n",
    "        self.resize = resize\n",
    "        self.grayscale = grayscale\n",
    "\n",
    "        # action_space.n equivalent\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # per env state\n",
    "        self.frame_buffers = [deque(maxlen=self.frame_stack) for _ in range(num_envs)]\n",
    "        self.last_obs = [None for _ in range(num_envs)]\n",
    "        self.dones = [False for _ in range(num_envs)]\n",
    "        self.total_rewards = [0.0 for _ in range(num_envs)]\n",
    "        self.current_life = [1 for _ in range(num_envs)]  # emulate lives\n",
    "\n",
    "        # initialize observations with zeros\n",
    "        for i in range(num_envs):\n",
    "            obs_i = self._reset_single(i)\n",
    "            self.last_obs[i] = obs_i\n",
    "\n",
    "    def _proc_obs(self, obs):\n",
    "        # obs comes as HWC RGB uint8\n",
    "        if self.grayscale:\n",
    "            obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        if self.resize is not None:\n",
    "            if self.grayscale:\n",
    "                obs = cv2.resize(obs, self.resize, interpolation=cv2.INTER_AREA)\n",
    "            else:\n",
    "                obs = cv2.resize(obs, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        if self.grayscale:\n",
    "            # shape (H, W) -> add channel\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "        else:\n",
    "            # HWC -> CHW\n",
    "            obs = np.transpose(obs, (2, 0, 1))\n",
    "        return obs.astype(np.uint8)\n",
    "\n",
    "    def _ensure_stack(self, env_id, obs_proc):\n",
    "        # Fill frame stack with the current processed frame\n",
    "        while len(self.frame_buffers[env_id]) < self.frame_stack:\n",
    "            self.frame_buffers[env_id].append(obs_proc)\n",
    "\n",
    "    def _get_stacked_obs(self, env_id):\n",
    "        # Stack into (C=frame_stack, H, W)\n",
    "        return np.stack(list(self.frame_buffers[env_id]), axis=0)\n",
    "\n",
    "    def _reset_single(self, env_id):\n",
    "        # Procgen reset returns batched obs when num_envs > 1\n",
    "        obs = self.env.reset()\n",
    "        # obs shape: (num_envs, H, W, C)\n",
    "        obs_i = obs[env_id]\n",
    "        obs_proc = self._proc_obs(obs_i)\n",
    "        self.frame_buffers[env_id].clear()\n",
    "        self._ensure_stack(env_id, obs_proc)\n",
    "        return self._get_stacked_obs(env_id)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset all envs\n",
    "        obs = self.env.reset()\n",
    "        for i in range(self.num_envs):\n",
    "            obs_i = obs[i]\n",
    "            obs_proc = self._proc_obs(obs_i)\n",
    "            self.frame_buffers[i].clear()\n",
    "            self._ensure_stack(i, obs_proc)\n",
    "            self.dones[i] = False\n",
    "            self.total_rewards[i] = 0.0\n",
    "            self.current_life[i] = 1\n",
    "            self.last_obs[i] = self._get_stacked_obs(i)\n",
    "        return [self.last_obs[i] for i in range(self.num_envs)]\n",
    "\n",
    "    def step(self, env_id, action):\n",
    "        # MaxAndSkip: take the same action skip times, accumulate reward, last obs\n",
    "        total_reward = 0.0\n",
    "        dead = False\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        a = int(action.item()) if hasattr(action, \"item\") else int(action)\n",
    "        for _ in range(self.skip):\n",
    "            obs, rewards, dones, infos = self.env.step(np.array([a] * self.num_envs))\n",
    "            # pick env_id\n",
    "            obs_i = obs[env_id]\n",
    "            r_i = float(rewards[env_id])\n",
    "            d_i = bool(dones[env_id])\n",
    "            info_i = infos[env_id] if isinstance(infos, (list, tuple)) else {}\n",
    "\n",
    "            total_reward += r_i\n",
    "            done = d_i\n",
    "            info = info_i\n",
    "\n",
    "            obs_proc = self._proc_obs(obs_i)\n",
    "            self.frame_buffers[env_id].append(obs_proc)\n",
    "\n",
    "            if done:\n",
    "                dead = True\n",
    "                break\n",
    "\n",
    "        # emulate lives info\n",
    "        info['lives'] = 0 if done else 1\n",
    "\n",
    "        next_obs = self._get_stacked_obs(env_id)\n",
    "        self.total_rewards[env_id] += total_reward\n",
    "        self.current_life[env_id] = info['lives']\n",
    "        self.last_obs[env_id] = next_obs\n",
    "        self.dones[env_id] = done\n",
    "\n",
    "        return next_obs, total_reward, dead, done, info\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        class _A:\n",
    "            def __init__(self, n): self.n = n\n",
    "        return _A(self.n_actions)\n",
    "\n",
    "\n",
    "class Environments():\n",
    "    def __init__(self, nb_actor):\n",
    "        # Choose a procgen game. Using \"miner\" to keep discrete small action space.\n",
    "        self.envs = [SimpleProcgenWrapper(game_name=\"miner\", num_envs=1, distribution_mode=\"easy\", skip=4, frame_stack=4, resize=(84, 84), grayscale=True) for _ in range(nb_actor)]\n",
    "        self.observations = [None for _ in range(nb_actor)]\n",
    "        self.current_life = [None for _ in range(nb_actor)]\n",
    "        self.done = [False for _ in range(nb_actor)]\n",
    "        self.total_rewards = [0 for _ in range(nb_actor)]\n",
    "        self.nb_actor = nb_actor\n",
    "\n",
    "        for env_id in range(nb_actor):\n",
    "            self.reset_env(env_id)\n",
    "\n",
    "    def len(self):\n",
    "        return self.nb_actor\n",
    "\n",
    "    def reset_env(self, env_id):\n",
    "        self.total_rewards[env_id] = 0\n",
    "        self.envs[env_id].reset()\n",
    "\n",
    "        # keep random noops by repeating a neutral action if exists\n",
    "        # in procgen, action 0 is usually \"noop\"\n",
    "        noop_action = 0\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            self.observations[env_id], reward, _, info_done, info = self.envs[env_id].step(noop_action)\n",
    "            self.total_rewards[env_id] += reward\n",
    "            self.current_life[env_id] = info['lives']\n",
    "            self.done[env_id] = info_done\n",
    "\n",
    "    def step(self, env_id, action):\n",
    "        next_obs, reward, dead, done, info = self.envs[env_id].step(action)\n",
    "        self.done[env_id] = done\n",
    "        self.total_rewards[env_id] += reward\n",
    "        self.current_life[env_id] = info['lives']\n",
    "        self.observations[env_id] = next_obs\n",
    "        return next_obs, reward, dead, done, info\n",
    "\n",
    "    def get_env(self):\n",
    "        # not used anymore, kept for compatibility\n",
    "        return self.envs[0]\n",
    "\n",
    "\n",
    "def PPO(envs, T=128, K=3, batch_size=32*8, gamma=0.99, device='cuda', gae_parameter=0.95,\n",
    "        vf_coeff_c1=1, ent_coef_c2=0.01, nb_iterations=40_000):\n",
    "\n",
    "        optimizer = torch.optim.Adam(actorcritic.parameters(), lr=2.5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=1., end_factor=0.0, total_iters=nb_iterations)\n",
    "\n",
    "        max_reward = 0\n",
    "        total_rewards = [[] for _ in range(envs.len())]\n",
    "        smoothed_rewards = [[] for _ in range(envs.len())]\n",
    "\n",
    "        for iteration in tqdm(range(nb_iterations)):\n",
    "            advantages = torch.zeros((envs.len(), T), dtype=torch.float32, device=device)\n",
    "            buffer_states = torch.zeros((envs.len(), T, 4, 84, 84), dtype=torch.float32, device=device)\n",
    "            buffer_actions = torch.zeros((envs.len(), T), dtype=torch.long, device=device)\n",
    "            buffer_logprobs = torch.zeros((envs.len(), T), dtype=torch.float32, device=device)\n",
    "            buffer_state_values = torch.zeros((envs.len(), T+1), dtype=torch.float32, device=device)\n",
    "            buffer_rewards = torch.zeros((envs.len(), T), dtype=torch.float32, device=device)\n",
    "            buffer_is_terminal = torch.zeros((envs.len(), T), dtype=torch.float16, device=device)\n",
    "\n",
    "            for env_id in range(envs.len()):\n",
    "                with torch.no_grad():\n",
    "                    for t in range(T):\n",
    "                        obs = torch.from_numpy(envs.observations[env_id] / 255.).unsqueeze(0).float().to(device)\n",
    "                        logits, value = actorcritic(obs)\n",
    "                        logits, value = logits.squeeze(0), value.squeeze(0)\n",
    "                        m = torch.distributions.categorical.Categorical(logits=logits)\n",
    "\n",
    "                        if envs.done[env_id]:\n",
    "                            action = torch.tensor([0]).to(device)  # noop on done to advance reset path\n",
    "                        else:\n",
    "                            action = m.sample()\n",
    "\n",
    "                        log_prob = m.log_prob(action)\n",
    "                        _, reward, dead, done, _ = envs.step(env_id, action)\n",
    "                        reward = np.sign(reward)\n",
    "\n",
    "                        buffer_states[env_id, t] = obs\n",
    "                        buffer_actions[env_id, t] = torch.tensor([action]).to(device)\n",
    "                        buffer_logprobs[env_id, t] = log_prob\n",
    "                        buffer_state_values[env_id, t] = value\n",
    "                        buffer_rewards[env_id, t] = reward\n",
    "                        buffer_is_terminal[env_id, t] = done\n",
    "\n",
    "                        if dead:\n",
    "                            if envs.total_rewards[env_id] > max_reward:\n",
    "                                max_reward = envs.total_rewards[env_id]\n",
    "                                torch.save(actorcritic.cpu(), f\"actorcritic_{max_reward}\")\n",
    "                                actorcritic.to(device)\n",
    "\n",
    "                            total_rewards[env_id].append(envs.total_rewards[env_id])\n",
    "                            envs.reset_env(env_id)\n",
    "\n",
    "                    buffer_state_values[env_id, T] = actorcritic(\n",
    "                        torch.from_numpy(envs.observations[env_id] / 255.).unsqueeze(0).float().to(device))[1].squeeze(0)\n",
    "\n",
    "                    for t in range(T-1, -1, -1):\n",
    "                        next_non_terminal = 1.0 - buffer_is_terminal[env_id, t]\n",
    "                        delta_t = buffer_rewards[env_id, t] + gamma * buffer_state_values[\n",
    "                            env_id, t+1] * next_non_terminal - buffer_state_values[env_id, t]\n",
    "                        if t == (T-1):\n",
    "                            A_t = delta_t\n",
    "                        else:\n",
    "                            A_t = delta_t + gamma * gae_parameter * advantages[env_id, t+1] * next_non_terminal\n",
    "                        advantages[env_id, t] = A_t\n",
    "\n",
    "            if (iteration % 400 == 0) and iteration > 0:\n",
    "                for env_id in range(envs.len()):\n",
    "                    smoothed_rewards[env_id].append(np.mean(total_rewards[env_id]) if len(total_rewards[env_id]) > 0 else 0.0)\n",
    "                    plt.plot(smoothed_rewards[env_id])\n",
    "                total_rewards = [[] for _ in range(envs.len())]\n",
    "                plt.title(\"Average Reward on Procgen\")\n",
    "                plt.xlabel(\"Training Epochs\")\n",
    "                plt.ylabel(\"Average Reward per Episode\")\n",
    "                plt.savefig('average_reward_on_procgen.png')\n",
    "                plt.close()\n",
    "\n",
    "            for epoch in range(K):\n",
    "                advantages_data_loader = DataLoader(\n",
    "                    TensorDataset(\n",
    "                        advantages.reshape(advantages.shape[0] * advantages.shape[1]),\n",
    "                        buffer_states.reshape(-1, buffer_states.shape[2], buffer_states.shape[3], buffer_states.shape[4]),\n",
    "                        buffer_actions.reshape(-1),\n",
    "                        buffer_logprobs.reshape(-1),\n",
    "                        buffer_state_values[:, :T].reshape(-1),\n",
    "                    ),\n",
    "                    batch_size=batch_size, shuffle=True,\n",
    "                )\n",
    "\n",
    "                for batch_advantages in advantages_data_loader:\n",
    "                    b_adv, obs, action_that_was_taken, old_log_prob, old_state_values = batch_advantages\n",
    "\n",
    "                    logits, value = actorcritic(obs)\n",
    "                    logits, value = logits.squeeze(0), value.squeeze(-1)\n",
    "                    m = torch.distributions.categorical.Categorical(logits=logits)\n",
    "                    log_prob = m.log_prob(action_that_was_taken)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)\n",
    "                    returns = b_adv + old_state_values\n",
    "\n",
    "                    policy_loss_1 = b_adv * ratio\n",
    "                    alpha = 1. - iteration / nb_iterations\n",
    "                    clip_range = 0.1 * alpha\n",
    "                    policy_loss_2 = b_adv * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                    policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                    value_loss1 = F.mse_loss(returns, value, reduction='none')\n",
    "                    value_loss2 = F.mse_loss(returns, torch.clamp(value, value - clip_range, value + clip_range), reduction='none')\n",
    "                    value_loss = torch.max(value_loss1, value_loss2).mean()\n",
    "\n",
    "                    loss = policy_loss + ent_coef_c2 * -(m.entropy()).mean() + vf_coeff_c1 * value_loss\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(actorcritic.parameters(), 0.5)\n",
    "                    optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda'\n",
    "    nb_actor = 8\n",
    "    envs = Environments(nb_actor)\n",
    "    actorcritic = ActorCritic(envs.envs[0].action_space.n).to(device)\n",
    "    PPO(envs, device=device)\n"
   ],
   "id": "b51e7722b3f833d9",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 314\u001B[0m\n\u001B[0;32m    312\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    313\u001B[0m nb_actor \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m\n\u001B[1;32m--> 314\u001B[0m envs \u001B[38;5;241m=\u001B[39m \u001B[43mEnvironments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnb_actor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    315\u001B[0m actorcritic \u001B[38;5;241m=\u001B[39m ActorCritic(envs\u001B[38;5;241m.\u001B[39menvs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    316\u001B[0m PPO(envs, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "Cell \u001B[1;32mIn[66], line 154\u001B[0m, in \u001B[0;36mEnvironments.__init__\u001B[1;34m(self, nb_actor)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, nb_actor):\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;66;03m# Choose a procgen game. Using \"miner\" to keep discrete small action space.\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs \u001B[38;5;241m=\u001B[39m [SimpleProcgenWrapper(game_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminer\u001B[39m\u001B[38;5;124m\"\u001B[39m, num_envs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, distribution_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124measy\u001B[39m\u001B[38;5;124m\"\u001B[39m, skip\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, frame_stack\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, resize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m84\u001B[39m, \u001B[38;5;241m84\u001B[39m), grayscale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservations \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_life \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n",
      "Cell \u001B[1;32mIn[66], line 154\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, nb_actor):\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;66;03m# Choose a procgen game. Using \"miner\" to keep discrete small action space.\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs \u001B[38;5;241m=\u001B[39m [\u001B[43mSimpleProcgenWrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgame_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mminer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_envs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistribution_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43measy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_stack\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m84\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m84\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrayscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservations \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_life \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n",
      "Cell \u001B[1;32mIn[66], line 53\u001B[0m, in \u001B[0;36mSimpleProcgenWrapper.__init__\u001B[1;34m(self, game_name, num_envs, distribution_mode, skip, frame_stack, resize, grayscale, start_level)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# initialize observations with zeros\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_envs):\n\u001B[1;32m---> 53\u001B[0m     obs_i \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reset_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_obs[i] \u001B[38;5;241m=\u001B[39m obs_i\n",
      "Cell \u001B[1;32mIn[66], line 86\u001B[0m, in \u001B[0;36mSimpleProcgenWrapper._reset_single\u001B[1;34m(self, env_id)\u001B[0m\n\u001B[0;32m     84\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# obs shape: (num_envs, H, W, C)\u001B[39;00m\n\u001B[1;32m---> 86\u001B[0m obs_i \u001B[38;5;241m=\u001B[39m \u001B[43mobs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_id\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     87\u001B[0m obs_proc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proc_obs(obs_i)\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe_buffers[env_id]\u001B[38;5;241m.\u001B[39mclear()\n",
      "\u001B[1;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T11:13:53.266289900Z",
     "start_time": "2025-10-08T11:13:21.871266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    nb_actor = 8\n",
    "    envs = Environments(nb_actor)\n",
    "\n",
    "    # backbone = CNNSimple(in_chans=4, img_size=84)  # CNN\n",
    "    backbone = SietBackbone(in_chans=4, img_size=84)   # or SietTiny\n",
    "\n",
    "    nb_actions = envs.envs[0].action_space.n\n",
    "\n",
    "    actorcritic = ActorCritic(backbone=backbone, action_dim=nb_actions).to(device)\n",
    "\n",
    "    PPO(envs, actorcritic=actorcritic, device=device)"
   ],
   "id": "ee9f18e9281e9e98",
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `procgen-coinrun` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameNotFound\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      5\u001B[0m nb_actor \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m\n\u001B[1;32m----> 6\u001B[0m envs \u001B[38;5;241m=\u001B[39m \u001B[43mEnvironments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnb_actor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# backbone = CNNSimple(in_chans=4, img_size=84)  # CNN\u001B[39;00m\n\u001B[0;32m      9\u001B[0m backbone \u001B[38;5;241m=\u001B[39m SietBackbone(in_chans\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, img_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m84\u001B[39m)   \u001B[38;5;66;03m# or SietTiny\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[57], line 6\u001B[0m, in \u001B[0;36mEnvironments.__init__\u001B[1;34m(self, nb_actor)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, nb_actor):\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_env() \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservations \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n",
      "Cell \u001B[1;32mIn[57], line 6\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, nb_actor):\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservations \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nb_actor)]\n",
      "Cell \u001B[1;32mIn[57], line 33\u001B[0m, in \u001B[0;36mEnvironments.get_env\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_env\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 33\u001B[0m     env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprocgen:procgen-coinrun-v0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m     env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mwrappers\u001B[38;5;241m.\u001B[39mRecordEpisodeStatistics(env)\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:689\u001B[0m, in \u001B[0;36mmake\u001B[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001B[0m\n\u001B[0;32m    686\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m    688\u001B[0m     \u001B[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001B[39;00m\n\u001B[1;32m--> 689\u001B[0m     env_spec \u001B[38;5;241m=\u001B[39m \u001B[43m_find_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env_spec, EnvSpec)\n\u001B[0;32m    693\u001B[0m \u001B[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001B[39;00m\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:533\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(env_id)\u001B[0m\n\u001B[0;32m    527\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    528\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the latest versioned environment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_env_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    529\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead of the unversioned environment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    530\u001B[0m     )\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 533\u001B[0m     \u001B[43m_check_version_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mError(\n\u001B[0;32m    535\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo registered env with id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    536\u001B[0m     )\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m env_spec\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:399\u001B[0m, in \u001B[0;36m_check_version_exists\u001B[1;34m(ns, name, version)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m get_env_id(ns, name, version) \u001B[38;5;129;01min\u001B[39;00m registry:\n\u001B[0;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 399\u001B[0m \u001B[43m_check_name_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\untracked_folder\\deep-reinforcement-learning-uzh\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:376\u001B[0m, in \u001B[0;36m_check_name_exists\u001B[1;34m(ns, name)\u001B[0m\n\u001B[0;32m    373\u001B[0m namespace_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in namespace \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ns \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    374\u001B[0m suggestion_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Did you mean: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msuggestion[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m suggestion \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 376\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mNameNotFound(\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnvironment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt exist\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnamespace_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msuggestion_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    378\u001B[0m )\n",
      "\u001B[1;31mNameNotFound\u001B[0m: Environment `procgen-coinrun` doesn't exist."
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
